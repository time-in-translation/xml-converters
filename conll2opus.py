import argparse
import re

from lxml import etree
import pyconll

from conll_utils import POS_TAGS


def convert_to_conll(in_file, out_file):
    """
    Strips of NER from the file generated by Stagger
    """
    results = []
    with open(in_file, 'r') as f:
        for line in f.readlines():
            line = line.strip()

            if line:
                parts = []

                for n, part in enumerate(line.split('\t')):
                    if n <= 4:
                        parts.append(part)
                    elif n == 12:
                        parts.append('_')
                        parts.append(part.split(':')[-1])
                parts += 3 * ['_']
                results.append('\t'.join(parts))
            else:
                results.append(line)

    with open(out_file, 'w') as f:
        for result in results:
            f.write(result)
            f.write('\n')


def find_paragraphs(in_file):
    with open(in_file, 'r') as f:
        contents = f.read()
        return [m.start() for m in re.finditer(r'\n\n', contents)]


def process_single(language, in_file, out_file, paragraph_starts):
    """
    Converts an CONLL-U file to the OPUS-xml format.
    """

    # Create counters for paragraphs and sentences
    i = 1
    j = k = 0
    sentence = None

    # Start a text element and add a first paragraph
    text = etree.Element('text')
    paragraph = etree.SubElement(text, 'p')
    paragraph.set('id', str(i))

    # Load the file and loop over the sentences
    sentences = pyconll.load_from_file(in_file)
    for sentence_conll in sentences:
        if paragraph_starts:
            for token in sentence_conll:
                if int(token.head) > paragraph_starts[0]:
                    i += 1
                    j = 0
                    paragraph = etree.SubElement(text, 'p')
                    paragraph.set('id', str(i))
                    sentence = None
                    paragraph_starts.pop(0)
                break

        if sentence is not None and len(sentence_conll) == 1:
            for token in sentence_conll:
                add_token(language, sentence, token, i, j, k + 1)

        else:
            j += 1
            sentence = etree.SubElement(paragraph, 's')
            sentence.set('id', 's{}.{}'.format(i, j))
            for k, token in enumerate(sentence_conll, start=1):
                add_token(language, sentence, token, i, j, k)

    tree = etree.ElementTree(text)
    tree.write(out_file, pretty_print=True, xml_declaration=True, encoding='utf-8')


def add_token(language, sentence, token, i, j, k):
    word = etree.SubElement(sentence, 'w')
    word.text = token.form
    word.set('id', 'w{}.{}.{}'.format(i, j, k))
    word.set('lem', token.lemma)
    word.set(POS_TAGS[language], token.upos)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('language', help='Language')
    parser.add_argument('file_in', help='Input file')
    parser.add_argument('file_txt', help='Plain-text file (to find paragraph boundaries)')
    parser.add_argument('file_out', help='Output file')
    args = parser.parse_args()

    tmp_file = 'conll.tmp'
    convert_to_conll(args.file_in, tmp_file)
    paragraphs = find_paragraphs(args.file_txt)
    process_single(args.language, tmp_file, args.file_out, paragraphs)
